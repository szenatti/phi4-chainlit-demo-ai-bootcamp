# Chainlit + Phi-4 (Ollama) Quickâ€‘start

A **noâ€‘frills â€œHelloÂ AIâ€ chat app** that runs locally with:

- **ChainlitÂ 2.5.5** â€“ instant chat UI for LLM apps  
- **Phiâ€‘4Â (14â€¯B) via Ollama** â€“ openâ€‘source model you can run onâ€‘device  
- **VSÂ Code** â€“ friendly editingÂ & debugging experience  

---

## 1 Â· Prerequisites

| Tool     | MinÂ Version | Notes                                                                 |
|----------|-------------|-----------------------------------------------------------------------|
| Python   |Â 3.9+        | Recommended: use `pyenv` or the VSÂ Code Python extension             |
| Git      |Â any         | For version control (optional)                                       |
| VSÂ Code  |Â 1.90+       | Install the **Python** extension                                     |
| Ollama   |Â 0.5.13+     | Brings its own runtime; installs Phiâ€‘4                               |
| Homebrew | latest      | macOS only â€“ simplifies installs (use Chocolatey/winget on Windows) |

### InstallÂ OllamaÂ + Phiâ€‘4

```bash
# macOS / Linux
brew install ollama
ollama serve &            # runs as a background service
ollama pull phi4          # downloads the model (~9Â GB)

# WindowsÂ (PowerShell)
winget install Ollama.Ollama
Start-Process "ollama" -ArgumentList "serve"
ollama pull phi4
```

---

## 2 Â· Create the project

```bash
git clone https://github.com/your-name/chainlit-phi4-demo
cd chainlit-phi4-demo
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt # Install dependencies
```

ProjectÂ layout:

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt      # autogenerated by pipâ€‘freeze (optional)
â””â”€â”€ .vscode
    â””â”€â”€ launch.json
```

---

## 3 Â· Write a **10â€‘line** ChainlitÂ appÂ (`app.py`)

```python
import chainlit as cl
import httpx

OLLAMA_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "phi4"

@cl.on_message
async def main(message: cl.Message):
    async with httpx.AsyncClient() as client:
        resp = await client.post(
            OLLAMA_URL,
            json={
                "model": MODEL_NAME,
                "messages": [{"role": "user", "content": message.content}],
                "stream": False,
            },
            timeout=60,
        )
    data = resp.json()
    await cl.Message(content=data["message"]["content"]).send()
```

---

## 4 Â· RunÂ & test

```bash
chainlit run app.py
```

The UI opens at **http://localhost:8000**.  
Ask anythingâ€”youâ€™re chatting with Phiâ€‘4 locally!

---

## 5 Â· VSÂ Code oneâ€‘click debugÂ (optional)

Create `.vscode/launch.json`:

```json
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Debug Chainlit",
      "type": "debugpy",
      "request": "launch",
      "module": "chainlit",
      "console": "integratedTerminal",
      "args": ["run", "app.py", "-w"],
      "envFile": "${workspaceFolder}/.env"
    }
  ]
}
```

- Press **F5** â†’ VSÂ Code starts Chainlit with hotâ€‘reload.  
- Set breakpoints in `app.py` to inspect the request/response cycle.

---

## 6 Â· NextÂ steps

1. ğŸ¨Â Explore Chainlit UI components (cards, widgets, file upload).  
2. ğŸ”—Â Integrate **LangChain** or **LlamaIndex** for tool/function calling.  
3. ğŸ“Â Add a `/data` folder and build your own RAG pipeline.  
4. ğŸš€Â Containerise with DockerÂ + GPU for faster inference.

---

## Troubleshooting

| Symptom                       | Fix                                                                     |
|--------------------------------|-------------------------------------------------------------------------|
| `connection refused`          | Ensure `ollama serve` is running.                                       |
| Long first response           | Phiâ€‘4 loads into RAM on first callâ€”give it ~30Â s.                       |
| Outâ€‘ofâ€‘memory error           | Use a quantised variant: `ollama pull phi4:Q5_K_M`.                    |
| VSÂ Code debugger stops early  | Check that `debugpy` is installed (`pip install debugpy`).             |

---

## License

This starter is MITâ€‘licensed. Phiâ€‘4 is licensed under MIT; Chainlit under Apacheâ€‘2.0.  
Happy hackingÂ ğŸ‰
