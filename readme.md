# Chainlit + Phi-4 (Ollama) Quick‑start

A **no‑frills “Hello AI” chat app** that runs locally with:

- **Chainlit 2.5.5** – instant chat UI for LLM apps  
- **Phi‑4 (14 B) via Ollama** – open‑source model you can run on‑device  
- **VS Code** – friendly editing & debugging experience  

---

## 1 · Prerequisites

| Tool     | Min Version | Notes                                                                 |
|----------|-------------|-----------------------------------------------------------------------|
| Python   | 3.9+        | Recommended: use `pyenv` or the VS Code Python extension             |
| Git      | any         | For version control (optional)                                       |
| VS Code  | 1.90+       | Install the **Python** extension                                     |
| Ollama   | 0.5.13+     | Brings its own runtime; installs Phi‑4                               |
| Homebrew | latest      | macOS only – simplifies installs (use Chocolatey/winget on Windows) |

### Install Ollama + Phi‑4

```bash
# macOS / Linux
brew install ollama
ollama serve &            # runs as a background service
ollama pull phi4          # downloads the model (~9 GB)

# Windows (PowerShell)
winget install Ollama.Ollama
Start-Process "ollama" -ArgumentList "serve"
ollama pull phi4
```

---

## 2 · Create the project

```bash
git clone https://github.com/your-name/chainlit-phi4-demo
cd chainlit-phi4-demo
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt # Install dependencies
```

Project layout:

```
.
├── app.py
├── requirements.txt      # autogenerated by pip‑freeze (optional)
└── .vscode
    └── launch.json
```

---

## 3 · Write a **10‑line** Chainlit app (`app.py`)

```python
import chainlit as cl
import httpx

OLLAMA_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "phi4"

@cl.on_message
async def main(message: cl.Message):
    async with httpx.AsyncClient() as client:
        resp = await client.post(
            OLLAMA_URL,
            json={
                "model": MODEL_NAME,
                "messages": [{"role": "user", "content": message.content}],
                "stream": False,
            },
            timeout=60,
        )
    data = resp.json()
    await cl.Message(content=data["message"]["content"]).send()
```

---

## 4 · Run & test

```bash
chainlit run app.py
```

The UI opens at **http://localhost:8000**.  
Ask anything—you’re chatting with Phi‑4 locally!

---

## 5 · VS Code one‑click debug (optional)

Create `.vscode/launch.json`:

```json
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Debug Chainlit",
      "type": "debugpy",
      "request": "launch",
      "module": "chainlit",
      "console": "integratedTerminal",
      "args": ["run", "app.py", "-w"],
      "envFile": "${workspaceFolder}/.env"
    }
  ]
}
```

- Press **F5** → VS Code starts Chainlit with hot‑reload.  
- Set breakpoints in `app.py` to inspect the request/response cycle.

---

## 6 · Next steps

1. 🎨 Explore Chainlit UI components (cards, widgets, file upload).  
2. 🔗 Integrate **LangChain** or **LlamaIndex** for tool/function calling.  
3. 📝 Add a `/data` folder and build your own RAG pipeline.  
4. 🚀 Containerise with Docker + GPU for faster inference.

---

## Troubleshooting

| Symptom                       | Fix                                                                     |
|--------------------------------|-------------------------------------------------------------------------|
| `connection refused`          | Ensure `ollama serve` is running.                                       |
| Long first response           | Phi‑4 loads into RAM on first call—give it ~30 s.                       |
| Out‑of‑memory error           | Use a quantised variant: `ollama pull phi4:Q5_K_M`.                    |
| VS Code debugger stops early  | Check that `debugpy` is installed (`pip install debugpy`).             |

---

## License

This starter is MIT‑licensed. Phi‑4 is licensed under MIT; Chainlit under Apache‑2.0.  
Happy hacking 🎉
